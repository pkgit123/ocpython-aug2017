{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peter Kim, OCPython Meetup - August 1, 2017\n",
    "PeopleSpace, Irvine, CA\n",
    "\n",
    "## Text Classification with Scikit-Learn\n",
    "Lending Club Dataset\n",
    ">Original Source: https://www.lendingclub.com/info/download-data.action\n",
    "><br>Kaggle Discussion: https://www.kaggle.com/wendykan/lending-club-loan-data\n",
    "\n",
    "## Inspiration Reference:\n",
    "When Words Sweat: Identifying Signals for Loan Default in the Text of Loan Applications\n",
    ">Netzer, Oded and Lemaire, Alain and Herzenstein, Michal, When Words Sweat: Identifying Signals for Loan Default in the Text of Loan Applications (November 6, 2016). Columbia Business School Research Paper No. 16-83. Available at SSRN: https://ssrn.com/abstract=2865327.  \n",
    "\n",
    "## Background References on Text Classification and Python/Scikit-Learn.\n",
    ">PyCon 2016 Tutorial from Data School, “Machine Learning with Text in scikit-learn (PyCon 2016),” by Kevin Markham on May 28, 2016. \n",
    "* YouTube Lecture Available at: https://www.youtube.com/watch?v=ZiKMIuYidY0  \n",
    "* Github Available at: https://github.com/justmarkham/pycon-2016-tutorial\n",
    ">\n",
    "><br> scikit-learn, “Working with Text Data”.  Available at: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "<br>\n",
    "><br> Kaggle tutorial, “Bag of Words Meets Bags of Popcorn.”  December 9 2014 – June 30, 2015.  Available at: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "<br>\n",
    "><br> scikit-learn, “Feature Extraction (Customizing the Vectorizer Class)”.  Available at: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    ">\n",
    "><br> Andreas Müller and Sarah Guido, \"Introduction to Machine Learning with Python,\" O'Reilly Media, October 2016.  Available at: http://shop.oreilly.com/product/0636920030515.do\n",
    ">\n",
    "><br> Sebastian Raschka, \"Python Machine Learning,\" Packt Publishing; 1 edition (September 23, 2015).  Available at: https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Pandas Dataframe with Lending Club data from 2007-2011.\n",
    "# The \"2007-2011.csv\" file has been cleaned-up from original \"LoanStats3a.csv\".  \n",
    "df_lendingclub = pd.read_csv(\"2007-2011.csv\")\n",
    "\n",
    "# How many rows and columns?\n",
    "df_lendingclub.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure it loaded correctly.\n",
    "df_lendingclub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are the columns?  \n",
    "print(list(df_lendingclub.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine first record, first 20 columns.\n",
    "df_lendingclub.iloc[0, 0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine first record, 'desc'.\n",
    "df_lendingclub['desc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine first record, 'loan_status'.\n",
    "df_lendingclub['loan_status'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Descriptions From Lending Club Data Dictionary:\n",
    ">'desc': Loan description provided by the borrower\n",
    "<br>\n",
    "<br>'loan_status': Current status of the loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new dataframe with 2 columns: 'desc' and 'loan_status'\n",
    "df_text = df_lendingclub[['desc', 'loan_status']].copy()\n",
    "\n",
    "# How many rows and columns?\n",
    "print(df_text.shape)\n",
    "\n",
    "# See if it loaded correctly.\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Status of Total Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total number of loans\n",
    "print(\"Total number of loans: \", df_text.shape[0])\n",
    "\n",
    "# How many are Fully Paid vs. Charged off?\n",
    "loan_status = df_text['loan_status'].value_counts()\n",
    "print(\"\\nTotal loans by loan status: \")\n",
    "print(loan_status)\n",
    "\n",
    "# What % of loans are Fully Paid?\n",
    "print(\"\\n% of total loans by loan status: \")\n",
    "print(loan_status/df_text.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loans  with Blank Descriptions by Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many 'desc' fields are blank?\n",
    "blank_desc = df_text['desc'].isnull().sum()\n",
    "print(\"Number of loans with blank descriptions: \", blank_desc)\n",
    "print(\"% of loans with blank descriptions: \", blank_desc/df_text.shape[0])\n",
    "\n",
    "# Of the loans with blank descriptions, what is loan status?\n",
    "blank_loan_status = df_text[df_text['desc'].isnull()]['loan_status'].value_counts()\n",
    "print(\"\\nLoans with blank descriptions by loan status: \")\n",
    "print(blank_loan_status)\n",
    "\n",
    "# What % of loans with blank descriptions are Fully Paid?\n",
    "print(\"\\n% of loans with blank descriptions by loan status: \")\n",
    "print(blank_loan_status/blank_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Loans with Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only include loans with descriptions, use dropna() method.\n",
    "df_text_desc = df_text.dropna()\n",
    "df_text_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many loans have a description?\n",
    "print(\"Number of loans with a description: \", df_text_desc.shape[0])\n",
    "\n",
    "# Loans with description by loan status?\n",
    "print(\"\\nLoans with descriptions by loan status: \")\n",
    "loan_status_desc = df_text_desc['loan_status'].value_counts()\n",
    "print(loan_status_desc)\n",
    "\n",
    "# What % of loans with descriptions are Fully Paid vs. Charged Off?\n",
    "print(\"\\n% of loans with descriptions by loan status: \")\n",
    "print(loan_status_desc/df_text_desc.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very Important to Balance the Classes\n",
    "Particularly for high-dimensional, highly-sparse datasets\n",
    "> Blagus, Rok and Lusa, Lara, “SMOTE for high-dimensional class-imbalanced data,” BMC Bioinformatics, March 22, 2013.  Available at: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106\n",
    ">\n",
    "><br>Blagus, Rok and Lusa, Lara, “Class prediction for high-dimensional class-imbalanced data,” BMC Bioinformatics, October 20, 2010.  Available at: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-523\n",
    ">\n",
    "><br>Alexander Yun-chung Liu, “The Effect of Oversampling and Undersampling on Classifying Imbalanced Text Datasets,” Thesis for M.S.E., The University of Texas at Austin, August 2004.  Available at: https://pdfs.semanticscholar.org/cade/435c88610820f073a0fb61b73dff8f006760.pdf\n",
    ">\n",
    "><br>Nick Becker Github page, “The Right Way to Oversample in Predictive Modeling,” December 23, 2016.  Available at: https://beckernick.github.io/oversampling-modeling/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bar chart showing different size of \"Fully Paid\" vs. \"Charged Off\"\n",
    "df_text_desc['loan_status'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In order to balance the classes, under-sample the majority class.\n",
    "Over-sampling the majority class can be done with SMOTE (Synthetic Minority Over-Sampling), \n",
    "but does not yield good results for high-dimensional, high-sparse datasets.  \n",
    "'''\n",
    "\n",
    "# Save Fully Paid (majority class) as separate dataframe.\n",
    "fully_paid = df_text_desc['loan_status'] == 'Fully Paid'\n",
    "df_fully_paid = df_text_desc[fully_paid]\n",
    "\n",
    "# Run pandas.sample method to use same number of samples as minority class.  \n",
    "num_samples = loan_status_desc[1]    # number of loans \"Charged Off\" (e.g. 3851)\n",
    "df_fp_undersample = df_fully_paid.sample(num_samples, random_state=1)  \n",
    "df_fp_undersample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save minority class as separate dataframe.\n",
    "charged_off = df_text_desc['loan_status'] == 'Charged Off'\n",
    "df_charged_off = df_text_desc[charged_off]\n",
    "df_charged_off.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate the \"Fully Paid\" (under-sampled) and \"Charged Off\" into one dataframe\n",
    "bal_frames = [df_fp_undersample, df_charged_off]\n",
    "df_balanced = pd.concat(bal_frames)\n",
    "df_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bar chart showing different size of \"Fully Paid\" vs. \"Charged Off\" (Balanced)\n",
    "df_balanced['loan_status'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the Content and Classes into train and test sets (20%).\n",
    "# To ensure that it splits it according to same class ratio, use \"stratify\" parameter.  \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced['desc'], df_balanced['loan_status'], \n",
    "                                                    random_state=1, test_size=0.2, stratify=df_balanced['loan_status'])\n",
    "\n",
    "# Print the size of each train and test datasets\n",
    "print('X training size: ', X_train.shape)\n",
    "print('y training size: ', y_train.shape)\n",
    "print('X test size: ', X_test.shape)\n",
    "print('y test size: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train is the loan descriptions from 'desc' field (80% of total)\n",
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_train is the class data, a.k.a. \"label data\" or \"target classes\" (80% of total)\n",
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The test_train_split output retains the data type as a panda series.  \n",
    "# Later we will run a list comprehension so this data, changing data type.\n",
    "# Either data type should work fine.\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The test data represents 20% of the total.  X_test is loan descriptions, y_test is class data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the y_test value_counts to verify confusion matrix\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text  \n",
    "Using techniques from Kaggle tutorial  and Scikit-Learn Tutorial.  (See references at the top).\n",
    "1. Create process_chars function\n",
    "1. List comprehension applying function to data.\n",
    "1. Create a LemmaTokenizer class\n",
    "1. Instantiate CountVectorizer object with LemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Create process_chars function.\n",
    "\n",
    "# Use stopwords from NLTK, but also add individual letters.\n",
    "stop_nltk = stopwords.words(\"english\")\n",
    "stop_nltk_plus = stop_nltk + [u'a',u'b',u'c',u'd',u'e',u'f',u'g',u'h',u'i',u'j',\n",
    "                         u'k',u'l',u'm',u'n',u'o',u'p',u'q',u'r',u's',u't',\n",
    "                         u'u',u'v',u'w',u'x',u'y',u'z']\n",
    "# In Python, searching a set is much faster than searching\n",
    "# a list, so convert the stop words to a set.  \n",
    "# Use this \"steps\" set in the below function.  \n",
    "stops = set(stop_nltk_plus)\n",
    "    \n",
    "# function to process documents\n",
    "def process_chars(input_text):\n",
    "    # Remove non-letters, and make lowercase\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", input_text)\n",
    "        \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()    \n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: List comprehension applying function to data.\n",
    "\n",
    "# Process the text of X_train.  Keep same name for simplicity.\n",
    "X_train = [process_chars(text_file) for text_file in X_train]\n",
    "\n",
    "# Process the text of X_train.  Keep same name for simplicity.\n",
    "X_test = [process_chars(text_file) for text_file in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Create a LemmaTokenizer class.  \n",
    "# Based on Scikit-Learn \"Feature Extraction\" page.\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Instantiate CountVectorizer object with LemmaTokenizer.\n",
    "# Based on Scikit-Learn \"Feature Extraction\" page.\n",
    "\n",
    "# instantiate CountVectorizer object, with LemmaTokenizer()\n",
    "count_vect_lemma = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1, 2), max_features=400, \n",
    "                                   max_df=0.90, stop_words='english')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1a: Vectorize Text into Document-Term-Matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Size of training dtm:  (6161, 1000)\n",
    "# Wall time: 20.2 s\n",
    "\n",
    "\n",
    "# Fit the vectorizer object to the X_train text data\n",
    "X_train_vect = count_vect_lemma.fit(X_train)\n",
    "\n",
    "# Transform the training text into a document-term-matrix\n",
    "X_train_dtm = X_train_vect.transform(X_train)\n",
    "\n",
    "print(\"Size of training dtm: \", X_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Size of test dtm:  (1541, 1000)\n",
    "# Wall time: 2.12 s\n",
    "\n",
    "\n",
    "# Transform the test text into a document-term-matrix (input fed into models)\n",
    "X_test_dtm = X_train_vect.transform(X_test)\n",
    "\n",
    "print(\"Size of test dtm: \", X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1b: In addition to CountVectorizer, use TfidfTransformer\n",
    "Let's try 3 different options:\n",
    "* TfidfTransformer(use_idf=True)\n",
    "* TfidfTransformer(use_idf=False)\n",
    "* Don't use TfidfTransformer at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a TfidfTransformer object, fit the X_train_dtm data, save as object.\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_dtm)\n",
    "\n",
    "# Transform the X_train_dtm data to the TfidfTransformer.  \n",
    "# Could name is something else, but keeping same name for simplicity.\n",
    "X_train_dtm = tf_transformer.transform(X_train_dtm)\n",
    "\n",
    "# What is the shape of the document-term-matrix?  (Should be same.)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1c: In addition to CountVectorizer, use TruncatedSVD\n",
    "TruncatedSVD used for dimensionality reduction, particular with sparse matrices (e.g. text matrices).\n",
    "\n",
    "When TruncatedSVD is used in conjunction with CountVectorizer and Tfidf, it is known as Latent Semantic Analysis (LSA).\n",
    "\n",
    "Scikit-Learn Documentation on TruncatedSVD\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "<br>\n",
    "><br>http://scikit-learn.org/stable/modules/decomposition.html\n",
    "<br>\n",
    "><br>http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD()\n",
    "\n",
    "normalizer = Normalizer(copy=True)\n",
    "\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X_train_dtm = lsa.fit_transform(X_train_dtm)\n",
    "\n",
    "X_test_dtm = lsa.transform(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Vectorize Text Using Hashing Vectorizer\n",
    "Usually HashingVectorizer is used to vectorize text documents that do not fit in memory.  But maybe it can be used as a dimensionality reduction technique, and improve prediction accuracy.\n",
    "<br>\n",
    "<br>Scikit-Learn Documentation on HashingVectorizer:\n",
    ">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer\n",
    "<br>\n",
    "><br>http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform an IDF normalization on the output of HashingVectorizer\n",
    "hasher = HashingVectorizer(n_features=400,tokenizer=LemmaTokenizer(),\n",
    "                           stop_words='english', non_negative=True,\n",
    "                           norm=None, binary=False)\n",
    "\n",
    "# Vectorizer uses pipeline to combine hasher and TfidfTransformer\n",
    "vectorizer = make_pipeline(hasher, TfidfTransformer(use_idf=False))\n",
    "\n",
    "# Create X_train_dtm\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Create X_test_dtm\n",
    "X_test_dtm = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Text Classification with Word2Vec\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An idea to explore in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Example Without GridSearchCV\n",
    "* In practice, the GridSearchCV class accomplishes same result, while tuning combinations of model parameters.\n",
    "* But to make it simpler to follow the workflow, here is an example of text classification step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Instatiate a classifier object.\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Step 2: \"Fit\" training data onto model, both data and labels.\n",
    "# The machine is \"learning\" how the training words match the label data (or \"classes\").  \n",
    "rf_clf.fit(X_train_dtm, y_train)\n",
    "\n",
    "# Step 3: Predict test data using model, only data (not labels)\n",
    "# Store results as predictions on test data ... next we will compare with real labels.  \n",
    "rf_test_predictions = rf_clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier: \")\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, rf_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(\"(rows are actual, columns are predictions)\")\n",
    "print(metrics.confusion_matrix(y_test, rf_test_predictions, labels=[\"Charged Off\", \"Fully Paid\"]))\n",
    "\n",
    "# print the Classification Report\n",
    "print(\"\\nClassification Report: \")\n",
    "print(metrics.classification_report(y_test, rf_test_predictions,target_names=[\"Charged Off\", \"Fully Paid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Technique #1: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 0.546664502516\n",
    "# bootstrap: True\n",
    "# class_weight: 'balanced'\n",
    "# n_estimators: 50\n",
    "# Wall time: 57.5 s\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# parameters \n",
    "parameters_rf = {'n_estimators': (10, 50, 100),                 # default 10\n",
    "                 'bootstrap': (True, False),                    # default true\n",
    "                  'class_weight': ('balanced', None)}           # default None\n",
    "\n",
    "# instantiate a classifier object\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_rf = GridSearchCV(rf, parameters_rf, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_rf = gs_rf.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(gs_rf.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_rf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier: \")\n",
    "\n",
    "# predict classification\n",
    "gs_rf_test_predictions = gs_rf.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_rf_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_rf_test_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test, gs_rf_test_predictions,target_names=[\"Charged Off\", \"Fully Paid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

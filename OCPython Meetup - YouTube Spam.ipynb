{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peter Kim, OCPython Meetup - August 1, 2017\n",
    "PeopleSpace, Irvine, CA\n",
    "\n",
    "## Outline\n",
    "1. Github page - download the Jupyter Notebooks or follow along (sorry for Python 2/3)\n",
    "> # https://github.com/pkgit123/ocpython-aug2017\n",
    "<br>\n",
    "\n",
    "    1. Jupyter Notebook: \"OCPython Meetup - YouTube Spam.ipynb\"\n",
    "        1. CSV file: \"Youtube04-Eminem.csv\"\n",
    "    1. Jupyter Notebook: \"OCPython Meetup - Lending Club.ipynb\"\n",
    "        1. CSV file: \"2007-2011.csv\"\n",
    "    1. Make sure installed: (I'm using Anaconda distribution of Python 3)\n",
    "        1. sklearn\n",
    "        1. pandas\n",
    "        1. re\n",
    "        1. nltk\n",
    "        1. matplotlib\n",
    "        1. numpy\n",
    "1. Introduction\n",
    "    1. Relatively new to Python, Data Science (other prior career)\n",
    "    1. Lewis University, M.S. in Data Science \n",
    "    1. First time posting to Github, talking at Python meetup\n",
    "    1. Poll Python usage (weekly, never, less often than weekly)\n",
    "    1. Poll Jupyter Notebook and Scikit-Learn usage\n",
    "1. Background and context (depending on pace)\n",
    "    1. Python\n",
    "    1. Jupyter Notebook\n",
    "    1. Scikit-Learn\n",
    "    1. Machine Learning\n",
    "    1. Text Classification\n",
    "\n",
    "\n",
    "## Text Classification with Scikit-Learn\n",
    "\n",
    "YouTube Spam Collection\n",
    "\n",
    "The YouTube Spam Collection v. 1 is a public set of YouTube comments that have been collected for spam research. It has five datasets composed by 1,956 real and non-encoded messages that were labeled as legitimate (ham) or spam.\n",
    ">Alberto, T.C., Lochter J.V., Almeida, T.A. TubeSpam: Comment Spam Filtering on YouTube. Proceedings of the 14th IEEE International Conference on Machine Learning and Applications (ICMLA'15), 1-6, Miami, FL, USA, December, 2015. (preprint).  Available at: http://dcomp.sor.ufscar.br/talmeida/papers/TCA_ICMLA15.pdf\n",
    ">\n",
    "> Data Source: http://dcomp.sor.ufscar.br/talmeida/youtubespamcollection/\n",
    ">\n",
    ">Also available at UCI Machine Learning Repository.\n",
    "><br>Source: http://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection#\n",
    "\n",
    "Download the zip files.  Make sure to include file \"Youtube04-Eminem.csv\" in same directory.\n",
    "\n",
    "## Note: This Jupyter notebook uses Python 2 (other one uses Python 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataframe with Eminem YouTube comments\n",
    "df_eminem = pd.read_csv(\"Youtube04-Eminem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_eminem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_eminem.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine dataframe \n",
    "{Class 0: ham, Class 1: spam}\n",
    "<br>'CONTENT' field is the comment, 'CONTENT' field is the class.\n",
    "<br>Ignore the other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine dataframe  {Class 0: ham, Class 1: spam}\n",
    "df_eminem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_eminem['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_eminem['CLASS'].value_counts()\n",
    "# df_eminem['CLASS'].value_counts().rename(index={1: 'Spam', 0: 'Ham'})\n",
    "vc_class = df_eminem['CLASS'].value_counts()\n",
    "vc_class = vc_class.rename(index={1: 'Spam', 0: 'Ham'})\n",
    "vc_class = vc_class.sort_index()\n",
    "vc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hist(bins=bins, grid=False); plt.xticks([0,1])\n",
    "vc_class.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the Content and Classes into train and test sets (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_eminem['CONTENT'], df_eminem['CLASS'], \n",
    "                                                    random_state=1, test_size=0.2, stratify=df_eminem['CLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'X training size: ', X_train.shape\n",
    "print 'y training size: ', y_train.shape\n",
    "print 'X test size: ', X_test.shape\n",
    "print 'y test size: ', y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Text into DTM (with Pre-Processing)\n",
    "* Pre-processing function\n",
    "* List comprehension with pre-processing function\n",
    "* LemmaTokenizer class\n",
    "* CountVectorizer object with LemmaTokenizer\n",
    "\n",
    "# Surprise: performs worse with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pre-processing from Kaggle tutorial\n",
    "\n",
    "# stop_nltk = stopwords.words(\"english\")\n",
    "# stop_nltk_plus = stop_nltk + [u'a',u'b',u'c',u'd',u'e',u'f',u'g',u'h',u'i',u'j',\n",
    "#                          u'k',u'l',u'm',u'n',u'o',u'p',u'q',u'r',u's',u't',\n",
    "#                          u'u',u'v',u'w',u'x',u'y',u'z']\n",
    "# # 4. In Python, searching a set is much faster than searching\n",
    "# #     a list, so convert the stop words to a set\n",
    "# stops = set(stop_nltk_plus)\n",
    "    \n",
    "# # function to process 10k documents\n",
    "# def process_10kchars(input_text):\n",
    "#     # 1. Remove non-letters, and make lowercase\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", input_text)\n",
    "        \n",
    "#     # 3. Convert to lower case, split into individual words\n",
    "#     words = letters_only.lower().split()    \n",
    "    \n",
    "#     # 5. Remove stop words\n",
    "#     meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "#     # 6. Join the words back into one string separated by space, \n",
    "#     # and return the result.\n",
    "#     return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Wall time: 22.1 s\n",
    "\n",
    "# # list comprehension on X_train_chunks, with process_10kchars\n",
    "# X_train_process = [process_10kchars(text_file) for text_file in X_train]\n",
    "\n",
    "# X_test_process = [process_10kchars(text_file) for text_file in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # create a new CountVectorizer with lemmatizer\n",
    "# class LemmaTokenizer(object):\n",
    "#     def __init__(self):\n",
    "#         self.wnl = WordNetLemmatizer()\n",
    "#     def __call__(self, doc):\n",
    "#         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "# count_vect_lemma = CountVectorizer(tokenizer=LemmaTokenizer())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # instantiate CountVectorizer object, with LemmaTokenizer()\n",
    "# count_vect_lemma = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1, 2), max_features=1000, \n",
    "#                                    max_df=0.90, stop_words='english') \n",
    "\n",
    "# # Fit the vectorizer object to the X_train text\n",
    "# X_train_vect = count_vect_lemma.fit(X_train_process, y_train)\n",
    "\n",
    "# # Transform the training text into a document-term-matrix\n",
    "# X_train_dtm = X_train_vect.transform(X_train_process)\n",
    "\n",
    "# print \"Size of training dtm: \", X_train_dtm.shape\n",
    "\n",
    "# # Transform the test text into a document-term-matrix (input fed into models)\n",
    "# X_test_dtm = X_train_vect.transform(X_test_process)\n",
    "\n",
    "# print \"Size of test dtm: \", X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize text into DTM (no pre-processing)\n",
    "* Leave text unprocessed, no lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer for document-term-matrix\n",
    "vect = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Fit the vectorizer object to the X_train text\n",
    "X_train_vect = vect.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training text into a document-term-matrix\n",
    "X_train_dtm = X_train_vect.transform(X_train)\n",
    "\n",
    "print \"Size of training dtm: \", X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The vectorizer object has method .get_feature_names()\n",
    "# to show the words and bi-grams used in feature vector.  \n",
    "print(vect.get_feature_names()[0:20])\n",
    "print(\"\\n\")\n",
    "print(vect.get_feature_names()[100:120])\n",
    "print(\"\\n\")\n",
    "print(vect.get_feature_names()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What does the actual dtm look like?  Convert to array (from sparse matrix).\n",
    "# Just one line (i.e. one document).  \n",
    "X_train_dtm.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the test text into a document-term-matrix (input fed into models)\n",
    "X_test_dtm = X_train_vect.transform(X_test)\n",
    "\n",
    "print \"Size of test dtm: \", X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Example Without GridSearchCV\n",
    "* In practice, the GridSearchCV class accomplishes same result, while tuning combinations of model parameters.\n",
    "* But to make it simpler to follow the workflow, here is an example of text classification step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Instatiate a classifier object.\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Step 2: \"Fit\" training data onto model, both data and labels.\n",
    "# The machine is \"learning\" how the training words match the label data (or \"classes\").  \n",
    "rf_clf.fit(X_train_dtm, y_train)\n",
    "\n",
    "# Step 3: Predict test data using model, only data (not labels)\n",
    "# Store results as predictions on test data ... next we will compare with real labels.  \n",
    "rf_test_predictions = rf_clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier Accuracy: \")\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, rf_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(\"(rows are actual, columns are predictions)\")\n",
    "print(metrics.confusion_matrix(y_test, rf_test_predictions))\n",
    "\n",
    "# print the Classification Report\n",
    "print(\"\\nClassification Report: \")\n",
    "print(metrics.classification_report(y_test, rf_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Technique #1: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# 0.95251396648\n",
    "# bootstrap: False\n",
    "# class_weight: 'balanced'\n",
    "# n_estimators: 50\n",
    "# Wall time: 40.1 s\n",
    "\n",
    "# parameters \n",
    "parameters_rf = {'n_estimators': (10, 50, 100),                 # default 10\n",
    "                 'bootstrap': (True, False),                          # default true\n",
    "                  'class_weight': ('balanced', None)}                # default None\n",
    "\n",
    "# instantiate a classifier object\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_rf = GridSearchCV(rf, parameters_rf, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_rf = gs_rf.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(gs_rf.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_rf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Random Forest Classifier: \"\n",
    "\n",
    "# predict classification\n",
    "gs_rf_test_predictions = gs_rf.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_rf_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_rf_test_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test, gs_rf_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify classes against the y_test data\n",
    "print \"Compare Target Class Label Counts to Classification Report\"\n",
    "print \"0: Ham, 1: Spam\\n\"\n",
    "print y_test.value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification Technique #2: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# parameters for MultinomialNB\n",
    "parameters_mnb = {'alpha': (0.001, 0.01, 0.1, 1.0, 10.0, 100.0),\n",
    "                 'fit_prior': (True, False)}    # default is True, use uniform if False\n",
    "\n",
    "# instantiate a MultinomialNB object\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_mnb = GridSearchCV(mnb, parameters_mnb, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_mnb = gs_mnb.fit(X_train_dtm, y_train)\n",
    "\n",
    "print \"Grid Search for MultinomialNB\"\n",
    "print \"Accuracy: \", gs_mnb.best_score_\n",
    "print \"alpha: \", gs_mnb.best_params_['alpha']\n",
    "print \"fit_prior: \", gs_mnb.best_params_['fit_prior']\n",
    "# for param_name in sorted(parameters.keys()):\n",
    "#     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Multinomial Naive Bayes: \"\n",
    "\n",
    "# predict classification\n",
    "gs_mnb_test_predictions = gs_mnb.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_mnb_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_mnb_test_predictions))\n",
    "\n",
    "# calculate predicted probabilities (poorly calibrated)\n",
    "gs_y_pred_test = gs_mnb.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "# print AUC\n",
    "print(metrics.roc_auc_score(y_test, gs_y_pred_test))\n",
    "\n",
    "# print classification report\n",
    "print(metrics.classification_report(y_test, gs_mnb_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify classes against the y_test data\n",
    "print \"Compare Target Class Label Counts to Classification Report\"\n",
    "print \"0: Ham, 1: Spam\\n\"\n",
    "print y_test.value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Technique #3: Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# 0.949720670391\n",
    "# alpha: 0.01\n",
    "# class_weight: 'balanced'\n",
    "# penalty: 'elasticnet'\n",
    "# Wall time: 23 s\n",
    "\n",
    "# parameters for SVM\n",
    "parameters_svm = {'penalty': (None, 'l1', 'l2', 'elasticnet'),  # default is 'l2'\n",
    "                  'alpha': (0.0001, 0.01, 1.0),                 # default 0.0001\n",
    "                  'class_weight': ('balanced', None)}           # default None\n",
    "\n",
    "# instantiate a SVM object\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_svm = GridSearchCV(svm, parameters_svm, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_svm = gs_svm.fit(X_train_dtm, y_train)\n",
    "\n",
    "print gs_svm.best_score_\n",
    "\n",
    "for param_name in sorted(parameters_svm.keys()):\n",
    "    print \"%s: %r\" % (param_name, gs_svm.best_params_[param_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Support Vector Machines: \"\n",
    "\n",
    "# predict classification\n",
    "gs_svm_test_predictions = gs_svm.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_svm_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_svm_test_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test, gs_svm_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify classes against the y_test data\n",
    "print \"Compare Target Class Label Counts to Classification Report\"\n",
    "print \"0: Ham, 1: Spam\\n\"\n",
    "print y_test.value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Technique #4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use GridSearchCV to tune model parameters\n",
    "\n",
    "# 0.95530726257\n",
    "# C: 1.0\n",
    "# class_weight: None\n",
    "# penalty: 'l2'\n",
    "# Wall time: 5.55 s\n",
    "\n",
    "# parameters \n",
    "parameters_lr = {'penalty': ('l1', 'l2'),               # default is 'l2'\n",
    "                  'C': (0.01, 1.0, 10.0, 100.0),            # default 1.0\n",
    "                  'class_weight': ('balanced', None)}                         # default None\n",
    "\n",
    "# instantiate a LogisticRegression object\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# instantiate a GridSearchCV object\n",
    "gs_lr = GridSearchCV(lr, parameters_lr, n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the training data\n",
    "gs_lr = gs_lr.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(gs_lr.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters_lr.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_lr.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Logistic Regression: \"\n",
    "\n",
    "# predict classification\n",
    "gs_lr_test_predictions = gs_lr.predict(X_test_dtm)\n",
    "\n",
    "# print accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, gs_lr_test_predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, gs_lr_test_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test, gs_lr_test_predictions,target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify classes against the y_test data\n",
    "print \"Compare Target Class Label Counts to Classification Report\"\n",
    "print \"0: Ham, 1: Spam\\n\"\n",
    "print y_test.value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparison of Models (w/o Pre-Processing, with GridSearch)\n",
    "\n",
    "(Precision, Recall, F1-Score for the Spam Class Only)\n",
    "\n",
    "| Model                    | Accuracy   | Precision    | Recall    | F1-Score    |\n",
    "|--------------------------|------------|--------------|-----------|-------------|\n",
    "| Random Forest            | 0.9555     | 0.98         | 0.94      | 0.96        |\n",
    "| Multinomial Naive Bayes  | 0.9333     | 0.92         | 0.96      | 0.94        |\n",
    "| Support Vector Machines  | 0.9778     | 1.00         | 0.96      | 0.98        |\n",
    "| Logistic Regression      | 0.9667     | 1.00         | 0.94      | 0.97        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
